#!/usr/bin/env python
"""
å…¨é¢æµ‹è¯•è„šæœ¬
"""
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent))

def test_imports():
    """æµ‹è¯•æ¨¡å—å¯¼å…¥"""
    print("=" * 50)
    print("æµ‹è¯•1: æ¨¡å—å¯¼å…¥")
    print("=" * 50)
    
    try:
        from storage.hbase_client import HBaseClient
        from storage.data_model import Document
        from search.tokenizer import Tokenizer
        from search.ranking import TFIDF, BM25
        from utils.text_processor import clean_text, tokenize
        from utils.file_handler import get_file_type
        print("âœ“ æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âœ— å¯¼å…¥å¤±è´¥: {e}")
        return False


def test_storage():
    """æµ‹è¯•å­˜å‚¨æ¨¡å—"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•2: å­˜å‚¨æ¨¡å—")
    print("=" * 50)
    
    try:
        from storage.hbase_client import HBaseClient
        from storage.data_model import Document
        from datetime import datetime
        
        client = HBaseClient()
        print(f"âœ“ HBaseå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ (ä½¿ç”¨HBase: {client.use_hbase})")
        
        # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
        test_doc = Document(
            url="https://test.ustc.edu.cn/test.html",
            title="æµ‹è¯•æ–‡æ¡£",
            content="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£çš„å†…å®¹",
            file_type="html",
            source="test.ustc.edu.cn"
        )
        
        # ä¿å­˜æ–‡æ¡£
        row_key = client.save_document(test_doc)
        print(f"âœ“ æ–‡æ¡£ä¿å­˜æˆåŠŸï¼ŒRow Key: {row_key}")
        
        # è¯»å–æ–‡æ¡£
        retrieved_doc = client.get_document(row_key)
        if retrieved_doc and retrieved_doc.url == test_doc.url:
            print("âœ“ æ–‡æ¡£è¯»å–æˆåŠŸ")
        else:
            print("âœ— æ–‡æ¡£è¯»å–å¤±è´¥")
        
        client.close()
        return True
    except Exception as e:
        print(f"âœ— å­˜å‚¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_tokenizer():
    """æµ‹è¯•åˆ†è¯å™¨"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•3: åˆ†è¯å™¨")
    print("=" * 50)
    
    try:
        from search.tokenizer import Tokenizer
        
        tokenizer = Tokenizer()
        test_texts = [
            "ä¸­ç§‘å¤§æ•™åŠ¡å¤„",
            "ä¸‹è½½ä¸­å¿ƒæ–‡ä»¶",
            "è´¢åŠ¡å¤„é€šçŸ¥"
        ]
        
        for text in test_texts:
            tokens = tokenizer.tokenize(text)
            print(f"  æ–‡æœ¬: {text} -> åˆ†è¯: {tokens}")
        
        print("âœ“ åˆ†è¯å™¨æµ‹è¯•æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âœ— åˆ†è¯å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_ranking():
    """æµ‹è¯•æ’åºç®—æ³•"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•4: æ’åºç®—æ³•")
    print("=" * 50)
    
    try:
        from search.ranking import TFIDF, BM25
        
        # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
        documents = [
            {'tokens': ['ä¸­ç§‘å¤§', 'æ•™åŠ¡å¤„', 'é€šçŸ¥']},
            {'tokens': ['è´¢åŠ¡å¤„', 'ä¸‹è½½', 'æ–‡ä»¶']},
            {'tokens': ['ä¸­ç§‘å¤§', 'æ‹›ç”Ÿ', 'ä¿¡æ¯']},
        ]
        
        query_tokens = ['ä¸­ç§‘å¤§', 'é€šçŸ¥']
        
        # æµ‹è¯•TF-IDF
        tfidf = TFIDF(documents)
        scores = []
        for i, doc in enumerate(documents):
            score = tfidf.calculate_tfidf(doc['tokens'], query_tokens)
            scores.append((i, score))
            print(f"  æ–‡æ¡£{i+1} TF-IDFåˆ†æ•°: {score:.4f}")
        
        # æµ‹è¯•BM25
        bm25 = BM25(documents)
        scores_bm25 = []
        for i, doc in enumerate(documents):
            score = bm25.calculate_bm25(doc['tokens'], query_tokens)
            scores_bm25.append((i, score))
            print(f"  æ–‡æ¡£{i+1} BM25åˆ†æ•°: {score:.4f}")
        
        print("âœ“ æ’åºç®—æ³•æµ‹è¯•æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âœ— æ’åºç®—æ³•æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_text_processor():
    """æµ‹è¯•æ–‡æœ¬å¤„ç†"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•5: æ–‡æœ¬å¤„ç†")
    print("=" * 50)
    
    try:
        from utils.text_processor import clean_text, extract_text_from_html, tokenize
        
        # æµ‹è¯•æ–‡æœ¬æ¸…ç†
        dirty_text = "  hello   world  \n\n  test  "
        cleaned = clean_text(dirty_text)
        print(f"  æ¸…ç†æ–‡æœ¬: '{dirty_text}' -> '{cleaned}'")
        
        # æµ‹è¯•HTMLæå–
        html = "<html><body><p>æµ‹è¯•å†…å®¹</p><script>alert('test')</script></body></html>"
        text = extract_text_from_html(html)
        print(f"  HTMLæå–: {text}")
        
        # æµ‹è¯•åˆ†è¯
        tokens = tokenize("ä¸­ç§‘å¤§æ•™åŠ¡å¤„é€šçŸ¥")
        print(f"  åˆ†è¯ç»“æœ: {tokens}")
        
        print("âœ“ æ–‡æœ¬å¤„ç†æµ‹è¯•æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âœ— æ–‡æœ¬å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_searcher():
    """æµ‹è¯•æœç´¢å¼•æ“ï¼ˆéœ€è¦æ•°æ®ï¼‰"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•6: æœç´¢å¼•æ“")
    print("=" * 50)
    
    try:
        from storage.hbase_client import HBaseClient
        from storage.data_model import Document
        from search.searcher import Searcher
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        client = HBaseClient()
        documents = client.get_all_documents(limit=5)
        client.close()
        
        if len(documents) == 0:
            print("âš  æ²¡æœ‰æ•°æ®ï¼Œè·³è¿‡æœç´¢æµ‹è¯•")
            print("  æç¤º: è¯·å…ˆè¿è¡Œçˆ¬è™« (python run_crawler.py)")
            return True
        
        print(f"  æ‰¾åˆ° {len(documents)} ä¸ªæ–‡æ¡£ï¼Œå¼€å§‹æµ‹è¯•æœç´¢...")
        searcher = Searcher()
        results = searcher.search("ä¸‹è½½", max_results=3)
        
        if results:
            print(f"  âœ“ æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            for i, (doc, score) in enumerate(results[:3], 1):
                print(f"    ç»“æœ{i}: [{score:.4f}] {doc.title[:50]}")
        else:
            print("  âš  æœç´¢æ— ç»“æœï¼ˆå¯èƒ½éœ€è¦å…ˆæ„å»ºç´¢å¼•ï¼‰")
        
        return True
    except Exception as e:
        print(f"âœ— æœç´¢å¼•æ“æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "=" * 50)
    print("å¼€å§‹å…¨é¢æµ‹è¯•")
    print("=" * 50)
    
    results = []
    
    results.append(("æ¨¡å—å¯¼å…¥", test_imports()))
    results.append(("å­˜å‚¨æ¨¡å—", test_storage()))
    results.append(("åˆ†è¯å™¨", test_tokenizer()))
    results.append(("æ’åºç®—æ³•", test_ranking()))
    results.append(("æ–‡æœ¬å¤„ç†", test_text_processor()))
    results.append(("æœç´¢å¼•æ“", test_searcher()))
    
    # æ€»ç»“
    print("\n" + "=" * 50)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 50)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for name, result in results:
        status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
        print(f"  {name}: {status}")
    
    print(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    else:
        print(f"\nâš  {total - passed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯")


if __name__ == '__main__':
    main()


