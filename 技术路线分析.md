# 技术路线分析

## 一、项目架构

```
数据采集层 → 数据存储层 → 搜索引擎层 → 用户界面层
```

## 二、技术选型

### 1. 数据采集层（爬虫）

**技术栈：**
- **Python 3.8+**：主要编程语言
- **Scrapy**：强大的爬虫框架，支持分布式爬取
- **BeautifulSoup4**：HTML解析
- **requests**：HTTP请求库
- **selenium**（可选）：处理JavaScript渲染的页面
- **pandas**：数据处理

**功能需求：**
- 爬取科大各学院和管理部门网站
- 重点爬取"下载中心"等文件专栏
- 支持多种文件格式（PDF、DOC、DOCX、XLS、XLSX等）
- 去重和增量更新
- 遵守robots.txt和合理延迟

### 2. 数据存储层（HBase）

**技术栈：**
- **HBase**：分布式列式数据库
- **Hadoop HDFS**：底层存储
- **happybase**：Python连接HBase的库
- **thrift**：HBase的Python客户端

**数据模型设计：**
- Row Key：文档ID（URL的hash或时间戳+序号）
- Column Family：
  - `info`: 存储元数据（url, title, content, file_type, crawl_time等）
  - `file`: 存储文件二进制数据（如果是文件）
  - `index`: 存储索引相关信息

### 3. 搜索引擎层

**技术栈（两种方案）：**

**方案A：使用Elasticsearch（推荐）**
- **Elasticsearch**：分布式搜索引擎
- **elasticsearch-py**：Python客户端
- 优点：开箱即用的搜索功能，支持相关性排序
- 缺点：需要额外部署ES服务

**方案B：自建倒排索引**
- **倒排索引**：基于Python实现
- **TF-IDF/BM25**：相关性评分算法
- **jieba**：中文分词
- 优点：完全可控，符合课程要求
- 缺点：需要自己实现更多功能

**推荐方案：方案B（自建），更能体现课程学习内容**

### 4. 用户界面层

**技术栈：**
- **Flask/FastAPI**：Web框架
- **HTML/CSS/JavaScript**：前端界面
- **Bootstrap**：UI框架（可选）

**功能：**
- 搜索框
- 结果展示（标题、摘要、来源、相关性分数）
- 分页
- 文件下载链接

## 三、项目结构

```
DataSystem/
├── crawler/              # 爬虫模块
│   ├── spiders/         # Scrapy爬虫
│   ├── pipelines.py     # 数据处理管道
│   ├── items.py         # 数据项定义
│   └── settings.py      # 爬虫配置
├── storage/             # 存储模块
│   ├── hbase_client.py  # HBase客户端
│   └── data_model.py    # 数据模型
├── search/              # 搜索引擎模块
│   ├── indexer.py       # 索引构建
│   ├── searcher.py      # 搜索实现
│   ├── tokenizer.py     # 分词器
│   └── ranking.py       # 排序算法
├── web/                 # Web界面
│   ├── app.py           # Flask应用
│   ├── templates/       # HTML模板
│   └── static/          # 静态文件
├── utils/               # 工具函数
│   ├── text_processor.py
│   └── file_handler.py
├── config/              # 配置文件
│   └── config.yaml
├── requirements.txt     # Python依赖
├── README.md
└── 作业要求.md
```

## 四、实现步骤

### 阶段1：环境搭建
1. 创建conda环境
2. 安装Hadoop和HBase
3. 安装Python依赖包
4. 配置HBase连接

### 阶段2：爬虫开发
1. 分析目标网站结构
2. 编写爬虫代码
3. 实现文件下载功能
4. 数据清洗和去重

### 阶段3：数据存储
1. 设计HBase表结构
2. 实现数据写入HBase
3. 实现数据读取接口

### 阶段4：搜索引擎
1. 实现中文分词
2. 构建倒排索引
3. 实现TF-IDF/BM25算法
4. 实现搜索和排序功能

### 阶段5：Web界面
1. 开发搜索API
2. 开发前端界面
3. 集成测试

### 阶段6：优化和测试
1. 性能优化
2. 功能测试
3. 准备演示

## 五、关键技术点

### 1. 中文分词
- 使用jieba进行中文分词
- 自定义词典（科大相关术语）

### 2. 相关性排序
- **TF-IDF**：词频-逆文档频率
- **BM25**：改进的TF-IDF算法
- 考虑标题权重、位置权重等

### 3. 去重策略
- URL去重
- 内容相似度去重（SimHash）

### 4. 增量更新
- 记录爬取时间
- 定期更新已爬取页面

## 六、依赖包清单

```
# 爬虫相关
scrapy>=2.11.0
beautifulsoup4>=4.12.0
requests>=2.31.0
selenium>=4.15.0

# 数据处理
pandas>=2.1.0
numpy>=1.24.0

# HBase
happybase>=1.2.0
thrift>=0.19.0

# 搜索引擎
jieba>=0.42.1
scikit-learn>=1.3.0  # 用于TF-IDF等算法

# Web框架
flask>=3.0.0
flask-cors>=4.0.0

# 工具库
python-dateutil>=2.8.2
pytz>=2023.3
```

## 七、注意事项

1. **遵守robots.txt**：爬取前检查网站的robots.txt
2. **合理延迟**：避免对服务器造成压力
3. **异常处理**：网络异常、解析异常等
4. **数据备份**：定期备份HBase数据
5. **性能优化**：大规模数据下的性能考虑


