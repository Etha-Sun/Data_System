# 项目实现总结

## 一、项目完成情况

本项目已完成所有核心功能模块的开发，包括：

### ✅ 1. 爬虫模块（Crawler）
- **文件位置**：`crawler/`
- **核心功能**：
  - 使用Scrapy框架实现分布式爬虫
  - 自动识别"下载中心"等文件专栏
  - 支持多种文件格式（PDF、DOC、DOCX、XLS、XLSX、TXT等）
  - 实现URL去重和增量更新
  - 自动提取文件文本内容

### ✅ 2. 存储模块（Storage）
- **文件位置**：`storage/`
- **核心功能**：
  - HBase客户端封装，支持连接HBase
  - 如果HBase不可用，自动fallback到本地文件存储
  - 数据模型定义（Document类）
  - 支持文档的增删改查操作
  - 支持倒排索引的存储

### ✅ 3. 搜索引擎模块（Search）
- **文件位置**：`search/`
- **核心功能**：
  - 中文分词（基于jieba，支持自定义词典）
  - 倒排索引构建
  - TF-IDF算法实现
  - BM25算法实现（默认使用）
  - 标题权重计算
  - 搜索结果排序

### ✅ 4. Web界面模块（Web）
- **文件位置**：`web/`
- **核心功能**：
  - Flask后端API
  - 响应式前端界面
  - 搜索功能
  - 按来源筛选
  - 结果展示（标题、摘要、来源、相关性分数）

### ✅ 5. 工具模块（Utils）
- **文件位置**：`utils/`
- **核心功能**：
  - 文本处理（HTML提取、文本清理、分词）
  - 文件处理（文件类型识别、文本提取）
  - SimHash去重算法

## 二、技术亮点

1. **智能Fallback机制**：如果HBase不可用，自动使用本地文件存储，保证系统可用性
2. **多算法支持**：同时实现TF-IDF和BM25算法，可在配置中切换
3. **标题权重**：标题匹配的文档获得更高相关性分数
4. **文件格式支持**：支持PDF、DOCX、TXT等多种格式的文本提取
5. **中文分词优化**：添加科大相关词汇到自定义词典

## 三、项目结构

```
DataSystem/
├── crawler/              # 爬虫模块
│   ├── spiders/         # Scrapy爬虫
│   ├── pipelines.py     # 数据处理管道
│   ├── items.py         # 数据项定义
│   └── settings.py      # 爬虫配置
├── storage/             # 存储模块
│   ├── hbase_client.py  # HBase客户端
│   └── data_model.py   # 数据模型
├── search/              # 搜索引擎模块
│   ├── indexer.py      # 索引构建
│   ├── searcher.py     # 搜索实现
│   ├── tokenizer.py    # 分词器
│   └── ranking.py      # 排序算法
├── web/                 # Web界面
│   ├── app.py          # Flask应用
│   └── templates/       # HTML模板
├── utils/               # 工具函数
│   ├── text_processor.py
│   └── file_handler.py
├── config/              # 配置文件
│   └── config.yaml
├── run_crawler.py       # 运行爬虫脚本
├── build_index.py       # 构建索引脚本
├── run_web.py          # 启动Web服务脚本
└── test_search.py      # 测试搜索脚本
```

## 四、使用流程

1. **环境准备**：激活conda环境，安装依赖
2. **运行爬虫**：`python run_crawler.py` 爬取数据
3. **构建索引**：`python build_index.py` 构建倒排索引
4. **启动服务**：`python run_web.py` 启动Web界面
5. **使用搜索**：访问 http://localhost:5000 进行搜索

## 五、配置说明

所有配置都在 `config/config.yaml` 中：

- **HBase配置**：连接信息、表名
- **爬虫配置**：下载延迟、并发数、User-Agent
- **搜索配置**：排序算法（tfidf/bm25）、结果数量
- **Web配置**：服务端口、调试模式

## 六、技术栈总结

| 模块 | 技术 | 说明 |
|------|------|------|
| 爬虫 | Scrapy | 强大的Python爬虫框架 |
| 存储 | HBase | 分布式列式数据库 |
| 分词 | jieba | 中文分词库 |
| 搜索 | 自建 | 倒排索引 + BM25/TF-IDF |
| Web | Flask | 轻量级Web框架 |
| 文件处理 | PyPDF2, python-docx | PDF和DOCX文本提取 |

## 七、注意事项

1. **HBase可选**：如果未安装HBase，系统会自动使用本地文件存储
2. **爬虫延迟**：已设置合理的下载延迟，遵守robots.txt
3. **文件提取**：某些格式（如.doc）可能无法提取文本，这是正常的
4. **数据量**：首次运行建议先用少量网站测试

## 八、扩展建议

1. **性能优化**：使用Redis缓存索引，提高搜索速度
2. **功能扩展**：添加搜索建议、热门搜索、搜索历史
3. **UI优化**：添加文档预览、高级搜索选项
4. **监控**：添加爬虫监控、搜索统计

## 九、实验报告建议

在实验报告中可以重点介绍：

1. **技术选型理由**：为什么选择自建搜索引擎而不是Elasticsearch
2. **HBase表设计**：Row Key设计、Column Family设计
3. **倒排索引实现**：如何构建和存储倒排索引
4. **相关性算法**：BM25算法的原理和实现
5. **Fallback机制**：如何实现HBase到本地存储的自动切换
6. **中文分词优化**：如何添加自定义词典提高分词准确性


