# 使用说明

## 一、环境准备

### 1. 激活conda环境

```bash
conda activate homework
```

### 2. 安装依赖（如果还没安装）

```bash
pip install -r requirements.txt
```

### 3. HBase配置（可选）

如果已安装HBase，请确保HBase服务正在运行。如果未安装HBase，系统会自动使用本地文件存储作为fallback。

## 二、运行步骤

### 步骤1：运行爬虫

爬取中科大各网站的文件和文档：

```bash
python run_crawler.py
```

或者使用Scrapy命令：

```bash
cd crawler
scrapy crawl ustc_spider
```

**注意**：
- 爬虫会自动识别"下载中心"等文件专栏
- 支持的文件格式：PDF、DOC、DOCX、XLS、XLSX、TXT等
- 爬取的数据会自动存储到HBase或本地文件

### 步骤2：构建索引

在爬取完数据后，需要构建倒排索引：

```bash
python build_index.py
```

这一步会：
- 加载所有文档
- 对文档进行分词
- 构建倒排索引
- 保存索引到HBase

### 步骤3：启动Web服务

启动搜索Web界面：

```bash
python run_web.py
```

然后在浏览器中访问：http://localhost:5000

## 三、功能说明

### 搜索功能

1. **基本搜索**：在搜索框输入关键词，点击搜索
2. **按来源筛选**：可以选择特定网站进行搜索
3. **结果排序**：结果按相关性分数降序排列
4. **结果展示**：显示标题、来源、文件类型、大小、相关度分数和内容摘要

### 技术特性

- **中文分词**：使用jieba进行中文分词
- **相关性排序**：使用BM25算法计算相关性
- **标题权重**：标题匹配的文档会获得更高分数
- **分布式存储**：使用HBase存储数据（如果可用）

## 四、配置说明

配置文件位于 `config/config.yaml`，可以修改：

- **HBase配置**：HBase连接信息
- **爬虫配置**：下载延迟、并发数等
- **搜索配置**：排序算法、结果数量等
- **Web配置**：服务端口、调试模式等

## 五、常见问题

### 1. HBase连接失败

如果HBase未安装或连接失败，系统会自动使用本地文件存储。数据会保存在 `data/storage/` 目录下。

### 2. 爬虫速度慢

可以在 `config/config.yaml` 中调整 `download_delay` 和 `concurrent_requests` 参数。

### 3. 搜索结果为空

- 确保已运行爬虫并爬取了数据
- 确保已运行 `build_index.py` 构建索引
- 检查搜索关键词是否正确

### 4. 文件提取失败

某些文件格式（如.doc）可能无法提取文本内容，这是正常的。系统会尝试提取，失败则跳过。

## 六、项目结构

```
DataSystem/
├── crawler/          # 爬虫模块
├── storage/          # 存储模块
├── search/          # 搜索引擎模块
├── web/             # Web界面
├── utils/           # 工具函数
├── config/          # 配置文件
├── data/            # 数据目录（自动创建）
├── run_crawler.py   # 运行爬虫
├── build_index.py   # 构建索引
└── run_web.py       # 启动Web服务
```

## 七、开发建议

1. **首次运行**：建议先用少量网站测试，确认功能正常后再全量爬取
2. **数据备份**：定期备份 `data/` 目录下的数据
3. **性能优化**：如果数据量大，可以考虑使用Redis缓存索引
4. **扩展功能**：可以添加搜索建议、热门搜索等功能


